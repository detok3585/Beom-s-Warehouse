# 190401실습



1. 제공된 VideoShop.sql파일을 이용하여 MS Sql Server에 VideoShop을 생성하고 Sqoop을 사용하여 VS_TAPE을 HDFS에 Import/Export하는 식을 작성하시오.

```
# DB Database List
$ ./bin/sqoop list-databases \
	--connect jdbc:sqlserver://70.12.114.149:1433 \
	--username 'sa' \
    --password 'Pa$$w0rd!3585'
```

![image](https://user-images.githubusercontent.com/46669509/55297543-16f20a00-5462-11e9-9a5c-afc7d70d6359.png)



```
# SQL 서버에서 불러오기
$ ./bin/sqoop import \
	--connect 'jdbc:sqlserver://70.12.114.149:1433;database=VideoShop'\
    --username 'sa' --password 'Pa$$w0rd!3585'\
    --table VS_CUSTOMER -m 1\
    --target-dir /user/hadoop/sqoopMsSqlToHdfs
    
```

```
# SQL 서버에 저장하기
$./bin/sqoop export\
	--connect 'jdbc:sqlserver://70.12.114.149:1433;database=VideoShop' \
	--username 'sa' \
	--password 'Pa$$w0rd!3585' \
    --table VS_CUSTOMER2 -m 1 \
    --export-dir /user/hadoop/sqoopMsSqlToHdfs/part-m-00000
    
```



2. Flume에서 Sequence Generator Source를 사용하여 HDFS에 로그를 생성하시오.

```
* Flume 설정 및 기타 

$ wget http://apache.mirror.cdnetworks.com/flume/1.9.0/apache-flume-1.9.0-bin.tar.gz
$ tar xvfz apache-flume-1.9.0-bin.tar.gz
$ cd apache-flume-1.9.0-bin
$ cp flume-env.sh.template flume-env.sh
$ vi flume-env.sh

* conf/flume-env.sh 설정
export JAVA_HOME=/usr/java/default

* hadoop-2.9.2 => *.jar 복사 
$ cd ~/hadoop-2.9.2
$ find . -name \*.jar | xargs cp -t ~/apache-flume-1.9.0-bin/lib

```



```
# conf/flume-conf.properties 설정 
$ vi flume-conf.properties

agent.sources = seqGenSrc
agent.channels = memoryChannel
agent.sinks = h1

agent.sources.seqGenSrc.type = seq
agent.sources.seqGenSrc.channels = memoryChannel

agent.channels.memoryChannel.type = memory
agent.channels.memoryChannel.capacity = 1000
agent.channels.memoryChannel.transactionCapacity = 100
agent.channels.memoryChannel.batchSize = 1

agent.sinks.h1.type = hdfs
agent.sinks.h1.hdfs.path = hdfs://server10:9000/user/hadoop/flume_logs/
agent.sinks.h1.hdfs.filePrefix = log
agent.sinks.h1.hdfs.fileType = DataStream
agent.sinks.h1.hdfs.writeFormat = Text
agent.sinks.h1.hdfs.batchSize = 100
agent.sinks.h1.hdfs.rollSize = 0
agent.sinks.h1.hdfs.rollCount = 10
agent.sinks.h1.channel = memoryChannel
```



```
# Flume 실행
$ ./bin/flume-ng agent \
	--conf-file conf/flume-conf.properties \
    -n agent -Dflume.root.logger=INFO,console
```



3. 전국 커피숍 년도별 폐업건수를 출력하시오.
     (coffee.csv파일)

```R
coffee = read.csv('~/hosthome/coffee.csv',fileEncoding = "CP949",encoding = "UTF-8")
coffee$dateOfclosure
close = coffee %>% filter(!is.na(dateOfclosure))
close$dateOfclosure = substr(close$dateOfclosure,0,4)
closetotal = close %>% group_by(dateOfclosure) %>% summarise(total = n())
plot(closetotal)
closetotal
```

![image](https://user-images.githubusercontent.com/46669509/55306051-b8428580-548d-11e9-814d-a67772dd7a3c.png)

![image](https://user-images.githubusercontent.com/46669509/55306034-a6f97900-548d-11e9-8a88-dc10ef607d52.png)



4. 연령대별로 암의 발생률을 구하고 그래프로 출력하시오.
     (cancer.csv파일)

```R
cancer = read.csv('~/hosthome/cancer.csv',fileEncoding = "CP949",encoding = "UTF-8")
age = cancer %>% 
  mutate(age = ifelse(age<30, "young",
                      ifelse(age<=59,"middle","old")
      ))
total = age %>% group_by(age) %>% summarise(totals = n())
total$totals = (total$totals/sum(total$totals))*100
barplot(total$totals,names=total$age)

```

![image](https://user-images.githubusercontent.com/46669509/55306711-12444a80-5490-11e9-91df-f3123d4c3d2c.png)

![image](https://user-images.githubusercontent.com/46669509/55306702-0c4e6980-5490-11e9-8f61-03db8ece39f3.png)

5. Apache Spark으로 WordCount 처리하시오.
     (ab40thv.txt파일)

```
bin/hadoop fs -mkdir /user/hadoop/spark_data
bin/hadoop fs -put ~/ab40thv.txt /user/hadoop/spark_data
bin/hadoop fs -put ~/baby_names.csv /user/hadoop/spark_data
bin/hadoop fs -ls /user/hadoop/spark_data

sbin/start-all.sh

./bin/pyspark --master spark://server10:7077  
```

```
 >>> data = sc.textFile("hdfs://server10:9000/user/hadoop/spark_data/ab40thv.txt")
 >>> data.collect()
 >>> data2 = data.flatMap(lambda line : line.split(" "))
 >>> data3 = data2.map(lambda word: (word,1))
 >>> data4 = data3.reduceByKey(lambda a, b: a+b)
 >>> data4.collect()
```







6. Apache Spark를 사용하여 제공하는 baby_names의 데이터를 이용하여 년도별,성별 아기 출생건수를 구하시오.
   (baby_names.csv파일)

```
>>> baby_names = sqlContext.read.format("com.databricks.spark.csv").options(header='true', inferschema='true').load('hdfs://server10:9000/user/hadoop/spark_data/baby_names.csv')

>>> baby_names.registerTempTable("baby_names")
# 모든 값 출력
>>> result = sqlContext.sql("select * from baby_names")
>>> result.show()

# 이름과 성별로 묶어 출력
>>> result2 = sqlContext.sql("select 'First Name',Sex,count(*) FROM baby_names group by 'First Name',Sex")
>>> result2.show()

# 년도와 성별로 묶어 출력
>>> result3 = sqlContext.sql("select Year,Sex,count(*) FROM baby_names group by Year,Sex")
>>> result3.show()
```



