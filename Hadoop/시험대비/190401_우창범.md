# 실습

[빅데이터 수집]

VideoShop데이터베이스의 VS_CUSTOMER테이블을 Sqoop을 사용하여 HDFS에 Import/Export하시오.  

1. DB의 리스트를 보자

```
$ ./bin/sqoop list-databases \
	--connect jdbc:sqlserver://70.12.114.149:1433 \
	--username 'sa' \
    --password 'Pa$$w0rd!3585'
```

![image](https://user-images.githubusercontent.com/46669509/55297543-16f20a00-5462-11e9-9a5c-afc7d70d6359.png)



2. import : SQL 서버에서 불러오기

```
$ ./bin/sqoop import \
	--connect 'jdbc:sqlserver://70.12.114.149:1433;database=VideoShop'\
    --username 'sa' --password 'Pa$$w0rd!3585'\
    --table VS_CUSTOMER -m 1\
    --target-dir /user/hadoop/importresult
```

#### 실행결과

![image](https://user-images.githubusercontent.com/46669509/55311573-40c92200-549e-11e9-88a5-4ebb270564cd.png)

![image](https://user-images.githubusercontent.com/46669509/55311613-5b030000-549e-11e9-9d52-021d7fb4d26a.png)

3.  export : SQL 서버에 저장하기

```
$ ./bin/sqoop export \
	--connect 'jdbc:sqlserver://70.12.114.149:1433;database=VideoShop' \
	--username 'sa' \
	--password 'Pa$$w0rd!3585' \
    --table New_VS_CUSTOMER -m 1 \
    --export-dir /user/hadoop/importresult/part-m-00000
```

#### 실행결과

![image](https://user-images.githubusercontent.com/46669509/55312031-73bfe580-549f-11e9-8761-8fc2e8221462.png)



[Microsoft R 서버를 활용한 빅데이터 처리 및 분석] 

전국 커피숍 년도별 폐업건수를 구하고 그래프로 출력하시오.  

```R
coffee = read.csv('~/hosthome/coffee.csv',fileEncoding = "CP949",encoding = "UTF-8")
coffee$dateOfclosure
close = coffee %>% filter(!is.na(dateOfclosure))
close$dateOfclosure = substr(close$dateOfclosure,0,4)
closetotal = close %>% group_by(dateOfclosure) %>% summarise(total = n())
plot(closetotal)
closetotal
```

#### 값 출력

![image](https://user-images.githubusercontent.com/46669509/55306051-b8428580-548d-11e9-814d-a67772dd7a3c.png)

#### 그래프 출력

![image](https://user-images.githubusercontent.com/46669509/55306034-a6f97900-548d-11e9-8a88-dc10ef607d52.png)



[HD Insight를 활용한 하둡, 스파크 엔지니어링] 

Apache Spark를 사용하여 WordCount를 구하시오. (ab40thv.txt)

1. 기본 세팅

```
$ bin/hadoop fs -mkdir /user/hadoop/spark_data
$ bin/hadoop fs -put ~/ab40thv.txt /user/hadoop/spark_data
$ bin/hadoop fs -put ~/baby_names.csv /user/hadoop/spark_data
$ bin/hadoop fs -ls /user/hadoop/spark_data

$ sbin/start-all.sh

$ ./bin/pyspark --master spark://server10:7077  
```

#### 실행화면

![image](https://user-images.githubusercontent.com/46669509/55312198-da450380-549f-11e9-94e3-9de0cdd4bf7f.png)



2. ab40thv.txt를 WordCount

```
 >>> ab40 = sc.textFile("hdfs://server10:9000/user/hadoop/spark_data/ab40thv.txt")
 >>> ab40.collect()
 >>> ab401 = ab40.flatMap(lambda line : line.split(" "))
 >>> ab402 = ab401.map(lambda word: (word,1))
 >>> ab403 = ab402.reduceByKey(lambda a, b: a+b)
 >>> ab403.collect()
```

#### 실행결과

![image](https://user-images.githubusercontent.com/46669509/55312174-c9948d80-549f-11e9-980f-982905a68a79.png)