# Hadoop

하둡의 에코시스템은 모두 자바



sqoop : 관계형 데이터베이스를 하둡의 HDFS에서 가져오거나 내보내는 기능을 하는 것

sqoop import : RDBMS에서 Sqoop을 사용해서 HDFS에 가져옴

sqoop export : HDFS에서 Sqoop을 사용해 RDBMS에 저장함



kafka : Streaming Data를 HDFS이나 HBASE로 적재할 수 있는 기능



flume : 분산 환경에서 대량의 로그 데이터를 효과적으로 수비하여, 합친 후 다른 곳으로 전송가능한 신뢰할 수 있는 서비스

Flume 지원환경 : linux, windows 지원

Flume 구성 요소 : Source, Channel, Sink 



Reliability(신뢰성 ) : 장애가 발생하더라도 유실없이 전송을 보장하는 것

Scalability(확장성) : 수평확장(Scale-Out)이 가능하여 분산수집이 가능한 구조



중요 구성요소

- HDFS
- Batch Processing



HDFS 구성요소

- Name Node
- Data Node



Hadoop Batch Processing 

- ResourceManager
- NodeManager



Hadoop Batch Processing Task

- MapTsk
- ReduceTask



Apache Spark

병렬로 동작하는 메모리 또는 디스크에 저장된 내결함성을 가진 요소들의 컬렉션 

-> RDD

함수

- Transformations
- Actions



.collect()

.take()



SparkContext

ClusterManager

Worker Node

Executer







Apache Hadaoop 2.x YARN





# R

%/% : 몫

%% : 나머지



#### 데이터 타입

Matrix : 동일한 데이터 타입

Dataframe : 다른 데이터 타입도 가능



R 단점

R 기본타입 : 논리형(logical), 문자형(character), 수치형(integer, ..)

초기화 : a <- 1

벡터선언 : c(1,2,3)

주석 처리 : #

str()

1부터 10까지 : a <- 1:10

빅데이터 3V



